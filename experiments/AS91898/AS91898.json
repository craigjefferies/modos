{
  "meta": {
    "as_code": "AS91898",
    "title": "Demonstrate understanding of a computer science concept",
    "version": "1",
    "credits": 3,
    "assessment_type": "External",
    "source_pdf": "as91898.pdf",
    "purpose": "Students show understanding of a chosen computer-science concept by describing how it works, how it is used, and what impacts or issues it raises."
  },
  "grade_descriptors": {
    "N1": "Evidence does not reach the minimum threshold for Achieved-level indicators.",
    "N2": "Some Achieved-level indicators are met, but not enough to satisfy all required Achieved criteria.",
    "A3": "All Achieved-level indicators are met; no Merit-level indicator meets the Merit threshold.",
    "A4": "All Achieved indicators are met and at least one Merit indicator is met, but overall performance is below the Merit threshold.",
    "M5": "All Achieved indicators are met and the Merit threshold is reached; no Excellence-level indicator meets the Excellence threshold.",
    "M6": "All Achieved indicators are met, all Merit indicators are met, and at least one Excellence indicator is met, but overall performance is below the Excellence threshold.",
    "E7": "All Achieved and Merit indicators are met and the Excellence threshold is reached, but not all Excellence indicators are met.",
    "E8": "All Achieved, Merit, and Excellence indicators are met."
  },
  "key_terms": [
    { "term": "mechanism", "definition": "A technique, algorithm, principle, protocol, or process that makes the concept work." },
    { "term": "impact", "definition": "An ethical, social, sustainability, human-factor, or future-proofing consequence of using the concept." },
    { "term": "machine learning (ML)", "definition": "A field of AI where computers learn patterns from data to perform tasks without being explicitly programmed with step-by-step rules." },
    { "term": "neural network (NN)", "definition": "A layered set of interconnected ‘neurons’ that adjust their connection weights during training to recognise complex patterns, often used for vision or speech tasks." },
    { "term": "natural language processing (NLP)", "definition": "The branch of AI that enables computers to understand, interpret, and generate human language in text or speech form." }
  ],
  "omis": [
    {
      "id": "identify_concept",
      "description": "Clearly states the chosen computer science concept by name.",
      "level": "A",
      "weight": 1,
      "detection_hint": "The concept must be clearly named (e.g., ‘artificial intelligence’, ‘neural networks’, ‘recommender systems’). Vague terms like ‘technology’ or ‘software’ are insufficient unless tied to the specific concept.",
      "success_examples": ["artificial intelligence", "neural network (NN)", "collaborative recommender system"],
      "source_ref": "AS PDF p1 Achievement Criteria"
    },
    {
      "id": "describe_usage",
      "description": "Describes how the concept is used, implemented, or occurs by linking a named mechanism to a real-world context in one explanation.",
      "level": "A",
      "weight": 1,
      "detection_hint": "Link mechanism + task + context in the same explanation (e.g., ‘A vision model scans camera frames to detect pedestrians’). Listing items separately is insufficient.",
      "success_examples": ["A vision model scans dash‑cam images to detect pedestrians.", "A supervised classifier detects hate speech on social media.", "An LLM summarises doctor notes into plain‑English discharge summaries."],
      "source_ref": "Moderator commentary bullets"
    },
    {
      "id": "explain_application",
      "description": "Explains how the concept is applied to address a specific opportunity or problem.",
      "level": "A",
      "weight": 1,
      "detection_hint": "State the problem/opportunity and how the concept helps (benefit or outcome). Vague claims like ‘it helps’ are insufficient.",
      "success_examples": ["Reduces road deaths by braking faster than humans.", "Filters hate speech to protect users.", "Removes jargon to improve patient understanding."],
      "source_ref": "AS PDF p2 §2 bullet 3"
    },
    {
      "id": "explain_mechanism",
      "description": "Describes a relevant mechanism by explaining what it does in context (role‑level).",
      "level": "A",
      "weight": 2,
      "detection_hint": "Name the mechanism and explain, in plain language, how it is trained (e.g., on labelled examples), what patterns it learns, and how it uses them to make predictions in the chosen context. No inner algorithm detail required.",
      "success_examples": [
        "Car safety: trained on labelled images, learns visual patterns, then predicts pedestrians so AEB can brake.",
        "Social media: trained on labelled posts, learns language patterns, then predicts hate‑speech labels for moderation.",
        "Healthcare: trained on clinical notes, learns phrasing patterns, then generates patient‑friendly summaries."
      ],
      "source_ref": "AS PDF p2 §2 bullet 4"
    },
    {
      "id": "articulate_advantage_or_limitation",
      "description": "States a clear, context‑linked advantage or limitation of using the concept.",
      "level": "A",
      "weight": 1,
      "detection_hint": "Give a specific pro or con tied to the context (e.g., ‘camera glare can trigger phantom braking’). Generic praise/criticism is insufficient.",
      "success_examples": ["Camera glare can cause phantom braking.", "LLMs may hallucinate incorrect medical dosages.", "Slang may be wrongly flagged as offensive."],
      "source_ref": "Moderator commentary & 2023 paper (a)(iii)"
    },
    {
      "id": "analyse_impact",
      "description": "Explains at least one significant impact of the concept by linking cause to effect.",
      "level": "M",
      "weight": 1,
      "detection_hint": "Use causal language (‘because’, ‘leads to’, ‘results in’) to connect the concept to an ethical/social/human‑factor/sustainability impact.",
      "success_examples": ["Because training data lacked dialect diversity, the model flagged Māori slang as hate speech.", "Storing dash‑cam footage online raises privacy risks because it reveals locations.", "Hallucinations can cause harm because errors are hard to trace."],
      "source_ref": "AS PDF p2 §5"
    },
    {
      "id": "compare_multiple_impacts",
      "description": "Compares or contrasts two or more impacts to show depth of understanding.",
      "level": "M",
      "weight": 1,
      "detection_hint": "Use contrast language (‘while’, ‘in contrast’, ‘more than’) to weigh impacts. Listing without comparison is insufficient.",
      "success_examples": ["While AI speeds triage, it also increases over‑trust in automated decisions.", "CNNs improve pedestrian safety but are less explainable than rule‑based systems.", "Chatbots reduce workload but risk leaking private health info."],
      "source_ref": "AS PDF p2 §5"
    },
    {
      "id": "discuss_key_issues",
      "description": "Explains a key problem or issue related to the concept and why it matters.",
      "level": "E",
      "weight": 1,
      "detection_hint": "Name a concrete issue (e.g., bias, hallucinations, misuse) and explain its consequence in context.",
      "success_examples": ["Algorithmic bias in facial recognition has led to wrongful arrests.", "LLM hallucinations in healthcare can produce fake dosages.", "Deepfakes and voice clones can be misused for scams."],
      "source_ref": "AS PDF comprehensive criterion"
    },
    {
      "id": "evaluate_issue_significance",
      "description": "Judges the significance or severity of the issue with reasoning or criteria.",
      "level": "E",
      "weight": 1,
      "detection_hint": "Make a justified judgement using evidence, comparison, or criteria (stakes, frequency, reversibility).",
      "success_examples": ["Bias in healthcare AI is more severe than in shopping apps because misdiagnoses harm patients.", "Hallucinated dosage information creates high‑stakes risk.", "Voice‑cloning scams can seriously erode public trust."],
      "source_ref": "AS PDF comprehensive criterion"
    },
    {
      "id": "propose_mitigation_or_future_proofing",
      "description": "Proposes a specific mitigation or future‑proofing strategy that addresses the issue.",
      "level": "E",
      "weight": 1,
      "detection_hint": "The strategy must logically match the issue. Generic ‘test more’ answers are insufficient without linkage.",
      "success_examples": ["Require human review for LLM outputs.", "Diversify training data to reduce cultural bias."],
      "source_ref": "AS PDF p2 §5 future‑proofing"
    }
  ],
  "aggregation_rules": {
    "method": "hierarchical",
    "gate_sequence": ["A", "M", "E"],
    "levels": {
      "A": { "required_fraction": 1.0 },
      "M": { "required_fraction": 0.6 },
      "E": { "required_fraction": 0.5 }
    },
    "sub_band_logic": {
      "Achieved": {
        "A3": { "extra_merit_indicators": 0 },
        "A4": { "extra_merit_indicators": "≥1" }
      },
      "Merit": {
        "M5": { "extra_excellence_indicators": 0 },
        "M6": { "extra_excellence_indicators": "≥1" }
      },
      "Excellence": {
        "E7": { "excellence_fraction": "<1.0" },
        "E8": { "excellence_fraction": "1.0" }
      }
    },
    "fallback_grade": "N1"
  },
  "governance": {
    "last_validated": "2025-08-11",
    "change_log": [
      "2025-07-10 – Tweaked detection hints for clarity, merged mechanism linkage, added advantage/limitation OMI, updated weights, added key terms.",
      "2025-07-13 – Updated detection hints and success examples to align with social media, car safety, and healthcare domains.",
      "2025-08-11 – Refined OMI descriptions to mirror NZQA bullets; simplified ‘explain_mechanism’ to role‑level and updated success examples to emphasise training → patterns → predictions."
    ]
  }
}
