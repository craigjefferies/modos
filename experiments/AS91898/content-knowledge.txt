# AS91898 Content Knowledge File – Simplified for OMI Alignment

---

### 1. Metadata
- Assessment Code and Title: AS91898 – Demonstrate understanding of a computer science concept
- Level and Credits: Level 2, 3 Credits
- Key Topics: Artificial Intelligence, Core Mechanisms (Supervised Learning, CNN, LLM, Recommenders), Metrics, Data pipeline, Ethical Impacts, Real-world Applications (Social Media, Car Safety, Healthcare)
- Last Updated: 2025-08-12
- Scaffolding Notes: Simple = Achieved (name, describe); Detailed = Merit (explain, connect cause/effect); Implication = Excellence (evaluate, judge, propose fix)
- Source Files Used: as91898.pdf, as91898.json, CS_Exam_AI_Resource.txt, 91898-cat-B-2023.pdf

---

### 2. Key Terms

| Term | Definition |
|------|------------|
| Artificial Intelligence | Software that mimics human thinking to perform tasks. |
| Mechanism | A process or method that makes the system work. |
| Impact | A consequence of using the concept (ethical, social, human-factor, etc). |
| Machine Learning (ML) | A method where computers learn patterns from data. |
| Neural Network (NN) | A model that learns to recognise patterns in data. |
| Natural Language Processing (NLP) | AI that processes and generates human language. |
| Supervised Learning | ML using labelled data to learn patterns. |
| Accuracy | How often the model is correct overall. |
| Precision | How often positive results are correct. |
| Precision@K | Accuracy of the top-K recommendations. |
| Bias | Systematic unfairness in outcomes. |
| Privacy | Risks from collecting or storing sensitive data. |
| Explainability | How clearly we can understand AI’s decisions. |
| Misuse | Harmful or unintended use of AI. |
| Data Pipeline | Steps to collect, clean, split, and evaluate data. |
| Future-proofing | Designing AI to stay safe and relevant over time. |
| Chatbot | An AI system that interacts using natural language. |

---

### 3. Knowledge Tables (Simplified)

#### 3.1 Core AI Mechanisms

| Mechanism | Simple Sentence | Example | Main Risk |
|-----------|-----------------|---------|-----------|
| **Supervised Classification** | Learns from labelled examples to sort new data into the right group. | Social media filters hate speech; Car reads road signs. | Bias if training data is unbalanced. |
| **CNN (Convolutional Neural Network)** | Finds patterns in images so it can recognise objects. | Car detects pedestrians; Medical AI scans X-rays. | Can be hard to explain why it made a decision. |
| **Transformer / LLM (NLP)** | Predicts the next word to understand or create text. | Summarises doctor notes; Chatbot answers questions. | Can make up false information (hallucinations). |
| **Collaborative Recommender** | Suggests things liked by people with similar interests. | TikTok “For You” feed; Health app suggests workouts. | Can create echo chambers or spread misinformation. |

---

#### 3.2 Metrics and Evaluation

| Concept | Simple Sentence | Example | Risk |
|---------|-----------------|---------|------|
| Accuracy | How often the model is right. | CNN classifies 95/100 correctly. | Can hide false positives or negatives. |
| Precision | How often “yes” predictions are correct. | Spam filter flags 50 emails, 45 are spam. | High precision doesn’t mean it finds all spam. |
| Perplexity (NLP) | Lower = smoother, more accurate text predictions. | Low perplexity in health summaries sounds more natural. | Not used for images or recommenders. |
| Precision@K | Checks if the top-K suggestions are relevant. | 4 of top 5 TikTok videos match your interests. | Doesn’t measure long-term value or fairness. |

---

#### 3.3 Data Pipeline

| Stage | Simple Sentence | Example | Risk |
|-------|-----------------|---------|------|
| Collection | Gather examples for training. | Road camera footage; Medical reports. | Bad data = bad model. |
| Cleaning | Remove errors or sensitive details. | Blur faces in images; Anonymise patient info. | Leaks or bias if skipped. |
| Split | Keep test data separate from training data. | 80% train, 20% test for tumour detection. | Using test data in training fakes results. |
| Evaluation | Check performance using metrics. | 95% accuracy for skin-cancer detection. | One metric alone can mislead. |

---

#### 3.4 Ethical and Social Impacts

| Issue | Simple Sentence | Example | Risk |
|-------|-----------------|---------|------|
| Bias | AI can be unfair to some groups. | Filter flags cultural slang as hate speech. | Discrimination or harm. |
| Privacy | AI can expose sensitive data. | Health chatbot stores user answers. | Legal and ethical issues. |
| Explainability | Hard to see why AI made a decision. | CNN brakes without clear reason. | Loss of trust. |
| Misuse | AI used in harmful ways. | Deepfakes for scams. | Public trust erodes. |

---

### 4. Common Misconceptions / Errors

- Confusing accuracy with precision.
- Naming a context without naming the mechanism.
- Explaining what AI does without linking to how it works at a role level.
- Mentioning an impact without cause/effect.
- Talking about “future-proofing” without linking it to a risk.
- Vague pros/cons not tied to examples.

---

### 5. Applied Scenarios

---

#### **Healthcare**
**ML (Supervised Classification)** – X-ray Classification  
- **Describe**: A supervised model classifies X-ray images as “healthy” or “tumour” using labelled examples.  
- **Explain**: Helps detect illness early **because** the AI spots patterns doctors might miss when busy.  
- **Discuss**: Risk of bias if training images mostly come from one age group. Fix by adding diverse examples.

**NN (CNN)** – Skin Cancer Detection  
- **Describe**: A CNN scans skin photos to detect early signs of melanoma.  
- **Explain**: Speeds up diagnosis **because** it can check thousands of images quickly.  
- **Discuss**: False positives may cause stress. Mitigate by having a doctor confirm results.

**NLP (LLM)** – Medical Note Summarisation  
- **Describe**: An LLM turns complex medical notes into patient-friendly summaries.  
- **Explain**: Saves time **because** patients can understand their care instructions.  
- **Discuss**: Hallucinations can add false info — require doctor review before sending to patients.

---

#### **Car Safety**
**ML (Supervised Classification)** – Road Surface Detection  
- **Describe**: A supervised model classifies road surfaces (wet, icy, dry) from sensor data.  
- **Explain**: Improves safety **because** braking adjusts to road conditions.  
- **Discuss**: Poor data from rare conditions could cause wrong predictions. Fix with simulated training data.

**ML (Supervised Classification)** – Driver Drowsiness Detection  
- **Describe**: A supervised model analyses steering patterns to detect driver drowsiness.  
- **Explain**: Prevents accidents **because** the car can alert the driver to rest.  
- **Discuss**: False alarms may annoy drivers — reduce by combining with eye-tracking sensors.

**NN (CNN)** – Pedestrian Detection  
- **Describe**: A CNN detects pedestrians in dashcam images to trigger automatic braking.  
- **Explain**: Reduces reaction time **because** AI sees objects faster than humans.  
- **Discuss**: Poor lighting can cause missed detections — improve with infrared sensors.

**NLP (Voice Recognition)** – Emergency Commands  
- **Describe**: An NLP system understands spoken commands like “call for help” after a crash.  
- **Explain**: Saves time **because** emergency services are contacted instantly.  
- **Discuss**: Background noise could cause errors — fix with noise-cancelling microphones.

---

#### **Social Media**
**ML (Supervised Classification)** – Hate Speech Detection  
- **Describe**: A supervised model detects hate speech in text posts using labelled training data.  
- **Explain**: Protects users **because** harmful content is removed.  
- **Discuss**: Bias risk if dialects are missing from the training set — fix with diverse examples.

**NN (CNN)** – Image Moderation  
- **Describe**: A CNN flags inappropriate images before they appear in feeds.  
- **Explain**: Keeps platform safe **because** harmful visuals are blocked automatically.  
- **Discuss**: False positives may hide harmless content — fix with human review of borderline cases.

**NLP (LLM)** – Thread Summarisation  
- **Describe**: An LLM summarises long posts or comment threads into a short overview.  
- **Explain**: Saves time **because** users can get the main points quickly.  
- **Discuss**: May miss important details — fix by letting users click to view the full thread.

**NLP (LLM)** – Misinformation Reduction  
- **Describe**: An LLM summarises breaking news posts into key facts to stop misinformation.  
- **Explain**: Improves accuracy **because** it highlights only confirmed information.  
- **Discuss**: Outdated training data could mislead — fix with live fact-checking sources.

---

### 6. Sources
- NZQA Assessment Standard PDF: as91898.pdf  
- OMI JSON file: as91898.json  
- Past Exams: 91898-cat-B-2023.pdf  
- Curriculum Resource: CS_Exam_AI_Resource.txt (2025 Content Guide)
